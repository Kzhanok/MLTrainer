{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFLOW_TRACKING_URI=sqlite:///mlruns.db\n"
     ]
    }
   ],
   "source": [
    "%env MLFLOW_TRACKING_URI=sqlite:///mlruns.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Adding dropout and normalization layers\n",
    "Study the pytorch documentation for:\n",
    "- Dropout https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "- normalization layers https://pytorch.org/docs/stable/nn.html#normalization-layers\n",
    "\n",
    "Experiment with adding dropout and normalization layers to your model. Some rough guidelines where to add them relative to Linear or Conv2d layers:\n",
    "- Dropout: after Linear or Conv2d layers. Often added after the last Linear layer *before* the output layer, but could occur more often.\n",
    "- Normalization layers: right after (blocks of) Linear or Conv2d layers, but before activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-21 17:34:07.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at C:\\Users\\leons\\.cache\\mads_datasets\\fashionmnist\u001b[0m\n",
      "\u001b[32m2024-09-21 17:34:07.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mFile already exists at C:\\Users\\leons\\.cache\\mads_datasets\\fashionmnist\\fashionmnist.pt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leons\\OneDrive\\Bureaublad\\School\\ML Trainer\\.venv\\lib\\site-packages\\mads_datasets\\factories\\torchfactories.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filepath)  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from loguru import logger\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "\n",
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "batchsize = 64\n",
    "preprocessor = BasePreprocessor()\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=batchsize, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()\n",
    "import torch\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-21 17:47:26.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_get_flattened_size\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mOutput shape after convolutions: torch.Size([1, 384, 2, 2])\u001b[0m\n",
      "\u001b[32m2024-09-21 17:47:26.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mFlattened size for the first Linear layer: 1536\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 28, 28]           1,280\n",
      "       BatchNorm2d-2          [-1, 128, 28, 28]             256\n",
      "              ReLU-3          [-1, 128, 28, 28]               0\n",
      "           Dropout-4          [-1, 128, 28, 28]               0\n",
      "         MaxPool2d-5          [-1, 128, 14, 14]               0\n",
      "            Conv2d-6          [-1, 256, 12, 12]         295,168\n",
      "       BatchNorm2d-7          [-1, 256, 12, 12]             512\n",
      "              ReLU-8          [-1, 256, 12, 12]               0\n",
      "           Dropout-9          [-1, 256, 12, 12]               0\n",
      "        MaxPool2d-10            [-1, 256, 6, 6]               0\n",
      "           Conv2d-11            [-1, 384, 4, 4]         885,120\n",
      "      BatchNorm2d-12            [-1, 384, 4, 4]             768\n",
      "             ReLU-13            [-1, 384, 4, 4]               0\n",
      "          Dropout-14            [-1, 384, 4, 4]               0\n",
      "        MaxPool2d-15            [-1, 384, 2, 2]               0\n",
      "          Flatten-16                 [-1, 1536]               0\n",
      "           Linear-17                  [-1, 128]         196,736\n",
      "      BatchNorm1d-18                  [-1, 128]             256\n",
      "             ReLU-19                  [-1, 128]               0\n",
      "          Dropout-20                  [-1, 128]               0\n",
      "           Linear-21                   [-1, 64]           8,256\n",
      "      BatchNorm1d-22                   [-1, 64]             128\n",
      "             ReLU-23                   [-1, 64]               0\n",
      "          Dropout-24                   [-1, 64]               0\n",
      "           Linear-25                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 1,389,130\n",
      "Trainable params: 1,389,130\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 4.67\n",
      "Params size (MB): 5.30\n",
      "Estimated Total Size (MB): 9.97\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from loguru import logger\n",
    "from torchsummary import summary\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, filters: int, units1: int, units2: int, input_size: tuple):\n",
    "        super().__init__()\n",
    "        self.in_channels = input_size[1]\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.MaxPool2d(kernel_size=2),  # Output size halved\n",
    "            nn.Conv2d(filters, filters*2, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(filters*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.MaxPool2d(kernel_size=2),  # Output size halved again\n",
    "            nn.Conv2d(filters*2, filters*3, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(filters*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.MaxPool2d(kernel_size=2),  # Output size halved once more\n",
    "        )\n",
    "\n",
    "        # Calculate the flattened size based on actual output shape after convolutions\n",
    "        flattened_size = self._get_flattened_size(input_size)\n",
    "        logger.info(f\"Flattened size for the first Linear layer: {flattened_size}\")\n",
    "\n",
    "        # Remove AdaptiveAvgPool2d, as the tensor is already reduced\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten the 2D to 1D\n",
    "            nn.Linear(flattened_size, units1),  # Input size should match the flattened size\n",
    "            nn.BatchNorm1d(units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(units1, units2),\n",
    "            nn.BatchNorm1d(units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(units2, 10)  # Output 10 classes\n",
    "        )\n",
    "\n",
    "    # This function calculates the flattened size after convolutions\n",
    "    def _get_flattened_size(self, input_size):\n",
    "        x = torch.ones(1, *input_size[1:], dtype=torch.float32)  # Add batch dimension\n",
    "        x = self.convolutions(x)\n",
    "        logger.info(f\"Output shape after convolutions: {x.shape}\")\n",
    "        return x.numel()  # Return the total number of elements (flattened size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "        x = self.dense(x)  # Forward to dense layers\n",
    "        return x\n",
    "\n",
    "# Define the model\n",
    "model = CNN(filters=128, units1=128, units2=64, input_size=(32, 1, 28, 28))\n",
    "\n",
    "# Print the model summary\n",
    "summary(model, input_size=(1, 28, 28), device='cpu')  # Correct input size for summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer.trainer import TrainerSettings, ReportTypes\n",
    "from mltrainer import metrics\n",
    "log_dir = Path(\"../../models/cnn\").resolve()\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True)\n",
    "accuracy = metrics.Accuracy()\n",
    "settings = TrainerSettings(\n",
    "    epochs=10,\n",
    "    metrics=[accuracy],\n",
    "    logdir=log_dir,\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TENSORBOARD],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-21 16:14:59.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to C:\\Users\\leons\\OneDrive\\Bureaublad\\School\\models\\cnn\\20240921-161459\u001b[0m\n",
      "\u001b[32m2024-09-21 16:14:59.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from mltrainer import metrics\n",
    "from mltrainer.trainer import Trainer\n",
    "optimizer = optim.Adam\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "accuracy = metrics.Accuracy()\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    settings=settings,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 93.60it/s]\n",
      "\u001b[32m2024-09-21 16:15:10.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.5297 test 0.5758 metric ['0.7952']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 95.37it/s]\n",
      "\u001b[32m2024-09-21 16:15:21.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.3300 test 0.4091 metric ['0.8631']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 95.38it/s]\n",
      "\u001b[32m2024-09-21 16:15:31.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.2884 test 0.3447 metric ['0.8783']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 94.23it/s]\n",
      "\u001b[32m2024-09-21 16:15:42.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.2588 test 0.4650 metric ['0.8264']\u001b[0m\n",
      "\u001b[32m2024-09-21 16:15:42.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.3447, current loss 0.4650.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 94.46it/s]\n",
      "\u001b[32m2024-09-21 16:15:52.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.2424 test 0.4476 metric ['0.8360']\u001b[0m\n",
      "\u001b[32m2024-09-21 16:15:52.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.3447, current loss 0.4476.Counter 2/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 92.68it/s]\n",
      "\u001b[32m2024-09-21 16:16:03.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 5 train 0.2268 test 0.3241 metric ['0.8883']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 93.43it/s]\n",
      "\u001b[32m2024-09-21 16:16:14.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 6 train 0.2170 test 0.2750 metric ['0.9009']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 93.61it/s]\n",
      "\u001b[32m2024-09-21 16:16:24.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 7 train 0.2068 test 0.2789 metric ['0.8995']\u001b[0m\n",
      "\u001b[32m2024-09-21 16:16:24.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.2750, current loss 0.2789.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 92.48it/s]\n",
      "\u001b[32m2024-09-21 16:16:35.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 8 train 0.1978 test 0.3041 metric ['0.8946']\u001b[0m\n",
      "\u001b[32m2024-09-21 16:16:35.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.2750, current loss 0.3041.Counter 2/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 92.77it/s]\n",
      "\u001b[32m2024-09-21 16:16:46.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 9 train 0.1918 test 0.2731 metric ['0.8985']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 10/10 [01:46<00:00, 10.65s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "Personal notes!\n",
    "\n",
    "- Dropout is tpyically used after the activation function and before the pooling function. This is because after the pooling function there are less activation neurons to drop out. so less weights to be influenced by the dropout. Dropout is designed to randomly set a fraction of the activations to zero. Applying it after the activation function ensures that the dropout is applied to the non-linear transformed features, which helps in regularizing the network more effectively.\n",
    "Also to maintain sctructure of the network.\n",
    "\n",
    "- Stabilizing Activations: By normalizing the activations before applying the non-linearity, you ensure that the inputs to the activation functions have a consistent distribution, which helps in stabilizing the training process.\n",
    "Improving Gradient Flow: Normalization helps in maintaining a stable gradient flow through the network, which can prevent issues like vanishing or exploding gradients.\n",
    "Accelerating Training: Normalized activations can lead to faster convergence during training, as the network can learn more efficiently.\n",
    "    Why Not After Activation?\n",
    "    Placing normalization layers after the activation functions can still work, but it might not be as effective. The primary goal of normalization is to control the distribution of the inputs to the activation functions, ensuring they are within a range that the activation functions can handle well.\n",
    "\n",
    "- Adding only batch normalization layer to the model, the accuracy improved to 92.3%! re adding the dropout layers decreased performance slightly but should improve generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Adding convolutional and pooling layers\n",
    "Previous lessons, you have started to experiment with you model.\n",
    "You might have tested the impact of the amount of units, the depth of layers and different learning rates.\n",
    "\n",
    "This lesson, we have added some new types of layers: convolutional and pooling layers.\n",
    "Experiment with adding these new layers.\n",
    "\n",
    "Also, have a look at the `ModuleList`: https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#modulelist\n",
    "It can be really useful to create a list of layers from a configfile, and then use that list to create your model.\n",
    "Instead of just adding a single layer, you could also add a block of layers (eg a Conv2d layer, followed by a ReLU layer, followed by a BatchNorm2d layer, followed by a MaxPool2d layer) and repeat that in a loop, adding it to the `ModuleList`.\n",
    "\n",
    "# 3. Improve your pipeline\n",
    "In addition to new layers, we have expanded our logging tools with MLFlow, so we currently can choose between gin-config, tensorboard and MLFlow.\n",
    "\n",
    "Expand your training pipeline you started in the previous lesson such that:\n",
    "\n",
    "- you can switch between models by changing a config file\n",
    "- you can test different hyperparameters by changing a config file\n",
    "- you automatically log settings: model picked, hyperparameters, metrics, etc. : use either gin-config, tensorboard or MLFlow to log that, or a combination, whatever you prefer.\n",
    "- Important: doing a master means you don't just start engineering a pipeline, but you need to reflect. Why do you see the results you see? What does this mean, considering the theory? Write down lessons learned and reflections, based on experimental results.\n",
    "- continuously improve your code: \n",
    "    - clean up your experimental environment, such that it doesnt get too messy\n",
    "    - automate the boring stuff: use a Makefile, use configfiles, automate logging, etc.\n",
    "    - use git: commit your changes often and with descriptive messages\n",
    "    - separate code for pipelines, configs, models, modeltraining and results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
